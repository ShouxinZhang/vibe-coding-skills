from __future__ import annotations

import argparse
import datetime as dt
import subprocess
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent
SKILL_DIR = SCRIPT_DIR.parent
PROJECT_ROOT = SKILL_DIR.parent.parent.parent
DOCS_DIR = PROJECT_ROOT / "docs"
LOGS_DIR = DOCS_DIR / "agent-logs"
INDEX_PATH = LOGS_DIR / "INDEX.md"
LOGBOOK_PATH = LOGS_DIR / "AGENT_LOGBOOK.md"


def _run_linux_date() -> str:
    try:
        result = subprocess.run(
            ["date", "+%Y-%m-%d %H:%M:%S %z"],
            capture_output=True,
            text=True,
            timeout=3,
            check=False,
        )
        if result.returncode == 0:
            value = (result.stdout or "").strip()
            if value:
                return value
    except Exception:
        pass
    return "unknown"


def _to_bool(value: str) -> int:
    normalized = value.strip().lower()
    if normalized in {"1", "true", "yes", "y"}:
        return 1
    if normalized in {"0", "false", "no", "n"}:
        return 0
    raise ValueError(f"invalid bool text: {value}")


def _to_bool_text(value: str) -> str:
    return "yes" if _to_bool(value) == 1 else "no"


def _collect_changed_files() -> list[str]:
    files: set[str] = set()
    try:
        tracked = subprocess.run(
            ["git", "-C", str(PROJECT_ROOT), "diff", "--name-only"],
            capture_output=True,
            text=True,
            timeout=5,
            check=False,
        )
        if tracked.returncode == 0:
            for line in (tracked.stdout or "").splitlines():
                path = line.strip()
                if path:
                    files.add(path)

        staged = subprocess.run(
            ["git", "-C", str(PROJECT_ROOT), "diff", "--cached", "--name-only"],
            capture_output=True,
            text=True,
            timeout=5,
            check=False,
        )
        if staged.returncode == 0:
            for line in (staged.stdout or "").splitlines():
                path = line.strip()
                if path:
                    files.add(path)

        untracked = subprocess.run(
            ["git", "-C", str(PROJECT_ROOT), "ls-files", "--others", "--exclude-standard"],
            capture_output=True,
            text=True,
            timeout=5,
            check=False,
        )
        if untracked.returncode == 0:
            for line in (untracked.stdout or "").splitlines():
                path = line.strip()
                if path:
                    files.add(path)
    except Exception:
        return []

    return sorted(files)


def _ensure_dirs() -> None:
    LOGS_DIR.mkdir(parents=True, exist_ok=True)


def _slug_now() -> str:
    now = dt.datetime.now()
    return now.strftime("%H%M%S")


def _partition_day() -> str:
    return dt.datetime.now().strftime("%Y-%m-%d")


def _partition_dir(day: str) -> Path:
    return LOGS_DIR / day


def _next_log_id() -> int:
    _ensure_dirs()
    max_id = 0
    for path in LOGS_DIR.glob("**/log-*.md"):
        if not path.is_file():
            continue
        parts = path.stem.split("-", 2)
        if len(parts) >= 2 and parts[1].isdigit():
            max_id = max(max_id, int(parts[1]))
    return max_id + 1


def _parse_title_id(path: Path) -> int | None:
    stem = path.stem
    if not stem.startswith("log-"):
        return None
    parts = stem.split("-", 2)
    if len(parts) < 2 or not parts[1].isdigit():
        return None
    return int(parts[1])


def _collect_log_files() -> list[Path]:
    _ensure_dirs()
    files: list[tuple[int, Path]] = []
    for path in LOGS_DIR.glob("**/log-*.md"):
        if not path.is_file():
            continue
        log_id = _parse_title_id(path)
        if log_id is not None:
            files.append((log_id, path))
    files.sort(key=lambda item: item[0], reverse=True)
    return [item[1] for item in files]


def _collect_day_dirs() -> list[Path]:
    _ensure_dirs()
    day_dirs: list[Path] = []
    for path in LOGS_DIR.iterdir():
        if path.is_dir() and len(path.name) == 10 and path.name.count("-") == 2:
            day_dirs.append(path)
    day_dirs.sort(key=lambda p: p.name, reverse=True)
    return day_dirs


def _extract_field(text: str, field: str) -> str:
    prefix = f"- {field}:"
    for line in text.splitlines():
        if line.startswith(prefix):
            return line[len(prefix) :].strip()
    return ""


def _write_index() -> None:
    logs = _collect_log_files()
    with INDEX_PATH.open("w", encoding="utf-8") as f:
        f.write("# Agent Logs Index\n\n")
        f.write("> Auto-generated by `.agents/skills/agent-logs/scripts/agent_logs.py`.\n\n")
        if not logs:
            f.write("暂无日志记录。\n")
            return

        f.write("| ID | Day | Time | Git | Isolated | Backup | File |\n")
        f.write("| --- | --- | --- | --- | --- | --- | --- |\n")
        for path in logs:
            text = path.read_text(encoding="utf-8")
            log_id = _parse_title_id(path)
            event_time = _extract_field(text, "Time")
            day = path.parent.name
            git_op = _extract_field(text, "Git Operation")
            isolated = _extract_field(text, "Modular Isolation")
            backup = _extract_field(text, "Backup Done")
            rel = path.relative_to(LOGS_DIR).as_posix()
            f.write(
                f"| {log_id} | {day} | {event_time} | {git_op} | {isolated} | {backup} | `{rel}` |\n"
            )


def _write_day_indexes() -> None:
    for day_dir in _collect_day_dirs():
        logs: list[tuple[int, Path]] = []
        for path in day_dir.glob("log-*.md"):
            log_id = _parse_title_id(path)
            if log_id is not None:
                logs.append((log_id, path))
        logs.sort(key=lambda item: item[0], reverse=True)

        index_path = day_dir / "INDEX.md"
        with index_path.open("w", encoding="utf-8") as f:
            f.write(f"# Agent Logs Partition {day_dir.name}\n\n")
            f.write("> Auto-generated by `.agents/skills/agent-logs/scripts/agent_logs.py`.\n\n")
            if not logs:
                f.write("暂无日志记录。\n")
                continue

            f.write("| ID | Time | Git | Isolated | Backup | File |\n")
            f.write("| --- | --- | --- | --- | --- | --- |\n")
            for log_id, path in logs:
                text = path.read_text(encoding="utf-8")
                event_time = _extract_field(text, "Time")
                git_op = _extract_field(text, "Git Operation")
                isolated = _extract_field(text, "Modular Isolation")
                backup = _extract_field(text, "Backup Done")
                f.write(
                    f"| {log_id} | {event_time} | {git_op} | {isolated} | {backup} | `{path.name}` |\n"
                )


def _write_log_file(log_id: int, payload: dict[str, object]) -> Path:
    day = _partition_day()
    day_dir = _partition_dir(day)
    day_dir.mkdir(parents=True, exist_ok=True)
    slug = _slug_now()
    file_name = f"log-{log_id:04d}-{slug}.md"
    path = day_dir / file_name
    files = payload["changed_files"]
    assert isinstance(files, list)

    with path.open("w", encoding="utf-8") as f:
        f.write(f"# Agent Log #{log_id}\n\n")
        f.write(f"- Time: {payload['event_time']}\n")
        f.write(f"- User Prompt: {payload['user_prompt']}\n")
        f.write(f"- Context Summary: {payload['context_summary']}\n")
        f.write(f"- Git Operation: {payload['git_operation_done']}\n")
        f.write(f"- Git Detail: {payload['git_operation_detail']}\n")
        f.write(f"- Modular Isolation: {payload['modular_isolation_done']}\n")
        f.write(f"- Backup Done: {payload['backup_done']}\n")
        f.write(f"- Backup Integrity OK: {payload['backup_integrity_ok']}\n")
        f.write(f"- Backup Consistency OK: {payload['backup_consistency_ok']}\n")
        f.write(f"- Backup Recoverability OK: {payload['backup_recoverability_ok']}\n")
        f.write(f"- Backup Note: {payload['backup_note']}\n\n")

        f.write("## Changed Files\n\n")
        if files:
            for item in files:
                f.write(f"- `{item}`\n")
        else:
            f.write("- (none)\n")

    return path


def cmd_add(args: argparse.Namespace) -> None:
    _ensure_dirs()
    if args.auto_files:
        changed_files = _collect_changed_files()
    else:
        changed_files = args.files or []

    payload = {
        "event_time": _run_linux_date(),
        "user_prompt": args.prompt,
        "context_summary": args.summary,
        "changed_files": changed_files,
        "git_operation_done": _to_bool_text(args.git_op),
        "git_operation_detail": args.git_detail or "",
        "modular_isolation_done": _to_bool_text(args.isolated),
        "backup_done": _to_bool_text(args.backup),
        "backup_integrity_ok": _to_bool_text(args.integrity_ok),
        "backup_consistency_ok": _to_bool_text(args.consistency_ok),
        "backup_recoverability_ok": _to_bool_text(args.recoverability_ok),
        "backup_note": args.backup_note or "",
    }

    log_id = _next_log_id()
    path = _write_log_file(log_id, payload)
    _write_index()
    _write_day_indexes()
    print(f"added log id={log_id} file={path}")


def cmd_get(args: argparse.Namespace) -> None:
    for path in _collect_log_files():
        log_id = _parse_title_id(path)
        if log_id == args.id:
            print(path.read_text(encoding="utf-8"))
            return

    if True:
        print(f"log id={args.id} not found")
        return


def cmd_list(args: argparse.Namespace) -> None:
    logs = _collect_log_files()[: args.limit]
    if not logs:
        print("no logs")
        return

    for path in logs:
        text = path.read_text(encoding="utf-8")
        log_id = _parse_title_id(path)
        event_time = _extract_field(text, "Time")
        git_op = _extract_field(text, "Git Operation")
        isolated = _extract_field(text, "Modular Isolation")
        backup = _extract_field(text, "Backup Done")
        rel = path.relative_to(LOGS_DIR).as_posix()
        print(
            f"id={log_id} time={event_time} git={git_op} "
            f"isolated={isolated} backup={backup} file={rel}"
        )


def cmd_export(_args: argparse.Namespace) -> None:
    _ensure_dirs()
    output = LOGBOOK_PATH
    logs = _collect_log_files()

    with output.open("w", encoding="utf-8") as f:
        f.write("# Agent Logbook\n\n")
        f.write("> Auto-generated by `.agents/skills/agent-logs/scripts/agent_logs.py export`.\n\n")
        if not logs:
            f.write("暂无日志记录。\n")
            print(f"exported: {output}")
            _write_index()
            _write_day_indexes()
            return

        for path in logs:
            rel = path.relative_to(LOGS_DIR).as_posix()
            f.write(f"## {rel}\n\n")
            f.write(path.read_text(encoding="utf-8"))
            f.write("\n\n")

    _write_index()
    _write_day_indexes()
    print(f"exported: {output}")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Agent execution logs manager")
    subparsers = parser.add_subparsers(dest="command", required=True)

    parser_add = subparsers.add_parser("add", help="Add one log record")
    parser_add.add_argument("--prompt", required=True, help="Complete original user prompt")
    parser_add.add_argument("--summary", required=True, help="Condensed LLM execution context summary")
    parser_add.add_argument("--files", nargs="*", help="Changed files list")
    parser_add.add_argument("--auto-files", action="store_true", help="Auto-collect changed files from git")
    parser_add.add_argument("--git-op", default="no", help="yes/no")
    parser_add.add_argument("--git-detail", default="", help="Git operation details")
    parser_add.add_argument("--isolated", default="no", help="Whether modular isolation is applied: yes/no")
    parser_add.add_argument("--backup", default="no", help="Whether backup operation is performed: yes/no")
    parser_add.add_argument("--integrity-ok", default="no", help="Backup integrity check passed: yes/no")
    parser_add.add_argument("--consistency-ok", default="no", help="Backup consistency check passed: yes/no")
    parser_add.add_argument("--recoverability-ok", default="no", help="Backup recoverability check passed: yes/no")
    parser_add.add_argument("--backup-note", default="", help="Backup note")

    parser_get = subparsers.add_parser("get", help="Get one log by id")
    parser_get.add_argument("id", type=int)

    parser_list = subparsers.add_parser("list", help="List recent logs")
    parser_list.add_argument("--limit", type=int, default=20)

    subparsers.add_parser("export", help="Export markdown logbook")

    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    if args.command == "add":
        cmd_add(args)
    elif args.command == "get":
        cmd_get(args)
    elif args.command == "list":
        cmd_list(args)
    elif args.command == "export":
        cmd_export(args)


if __name__ == "__main__":
    main()
